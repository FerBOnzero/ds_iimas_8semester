{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f875c543-6344-4aeb-ae3a-de4a8823a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List, Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0890c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/58/b8/51b956c2da9729390a3080397cd2f31171394543af7746681466e372f69a/torch-2.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.2.0-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\alumno\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.0-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "   ---------------------------------------- 0.0/198.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/198.6 MB 1.3 MB/s eta 0:02:32\n",
      "   ---------------------------------------- 0.4/198.6 MB 3.7 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 1.3/198.6 MB 8.9 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/198.6 MB 19.3 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 6.1/198.6 MB 28.0 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 6.7/198.6 MB 23.8 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 7.8/198.6 MB 24.8 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 9.7/198.6 MB 25.9 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.6/198.6 MB 38.6 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 12.4/198.6 MB 36.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 13.7/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 15.3/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 18.1/198.6 MB 38.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 21.6/198.6 MB 43.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 26.7/198.6 MB 93.0 MB/s eta 0:00:02\n",
      "   ------ -------------------------------- 31.5/198.6 MB 110.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 36.5/198.6 MB 93.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------ 41.4/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   --------- ----------------------------- 46.3/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   ---------- ---------------------------- 51.3/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   ----------- --------------------------- 56.2/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   ------------ -------------------------- 61.2/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   ------------ -------------------------- 66.2/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 71.3/198.6 MB 93.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------ 76.2/198.6 MB 110.0 MB/s eta 0:00:02\n",
      "   --------------- ----------------------- 81.2/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   ---------------- ---------------------- 86.2/198.6 MB 108.8 MB/s eta 0:00:02\n",
      "   ----------------- --------------------- 91.2/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------ -------------------- 96.3/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------- ------------------- 101.4/198.6 MB 93.9 MB/s eta 0:00:02\n",
      "   -------------------- ----------------- 106.5/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------- ---------------- 111.7/198.6 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------- --------------- 116.7/198.6 MB 110.0 MB/s eta 0:00:01\n",
      "   ----------------------- -------------- 122.0/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------ ------------- 127.1/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------- ------------ 132.2/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------- ----------- 137.3/198.6 MB 110.0 MB/s eta 0:00:01\n",
      "   --------------------------- ---------- 142.5/198.6 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------------- --------- 147.7/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------------------------- -------- 152.8/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------ ------- 158.1/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------ 163.2/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ----- 168.4/198.6 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 170.3/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.1/198.6 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- --- 180.7/198.6 MB 110.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 184.4/198.6 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.1/198.6 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.8/198.6 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 198.6/198.6 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed torch-2.2.0 typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24615be4-dd41-4348-afd2-4a01a9becea8",
   "metadata": {},
   "source": [
    "# 1. Funciones de Activación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c8b87-9831-4540-bbfb-c7bcf18ccb06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Implementar Funciones de Activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c746ece3-e0f7-4c5e-8c59-ceceb5a7933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua utilizando la función sigmoide los puntos x.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función\n",
    "    return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bbbcc-50b3-4e36-9491-d57e47a03e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title(\"Función de Activación Sigmoide\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"σ(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3964e72f-010d-46e6-90f2-3400608eb937",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtanh\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evalua utilizando la función tangente hiperbólica los puntos x.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# TODO: Completa la función.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# NOTA: No utilices la función np.tanh de NumPy.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua utilizando la función tangente hiperbólica los puntos x.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función.\n",
    "    # NOTA: No utilices la función np.tanh de NumPy.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2da45d-fbcc-4acd-beab-328810907c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, tanh(x))\n",
    "plt.title(\"Función de Activación Tanh\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"tanh(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15068d-d486-4578-b0ac-144573e50eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua utilizando la función ReLU los puntos x.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c72117-abdf-4f69-8900-427501057bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, relu(x))\n",
    "plt.title(\"Función de Activación ReLU\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"ReLU(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb9a56-63b8-4023-bd5a-0a2dfb80bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua utilizando la función softmax los puntos x.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c1110-962f-4b76-a0cc-12ed7e216c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5, 2.0, 0.1, 1.5])\n",
    "plt.bar(range(len(x)), softmax(x))\n",
    "plt.title(\"Función de Activación Softmax\")\n",
    "plt.xlabel(\"Índice\")\n",
    "plt.ylabel(\"Probabilidad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5be53-655d-4bb9-805c-44c302c5f493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 Implementar Derivadas de Funciones de Activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fca0e8-c762-439a-9a3d-d49314ba4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua los puntos x utilizando la derivada de la función sigmoide.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e249e8-2c8e-40c7-a387-4ec93dddaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid_derivative(x))\n",
    "plt.title(\"Derivada de Función Sigmoide\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"σ'(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5dd0a8-9241-4ecc-a476-9e156515edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua los puntos x utilizando la derivada de la función tangente hiperbólica.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fce838-7a18-4006-a163-93e2659233e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, tanh_derivative(x))\n",
    "plt.title(\"Derivada de la Función Tangente Hiperbólicda\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"tanh'(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa6219-ec81-45b2-b8a8-fb14157b9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Evalua los puntos x utilizando la derivada de la función ReLU.\n",
    "    :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "    :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "    \"\"\"\n",
    "    # TODO: Completa la función.\n",
    "    # NOTA: Puedes usar la función np.where\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c4553-4b0a-4391-88c4-7459156e9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, relu_derivative(x))\n",
    "plt.title(\"Derivada de Función ReLU\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"ReLU'(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809fc6b-dee4-41f6-88b6-f73bb2b0a584",
   "metadata": {},
   "source": [
    "# 2. Implementación de Red Neuronal Artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5ecc9-8e70-41d6-b58e-36cea04ec7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNeuralNetwork:\n",
    "    def __init__(self, layers, activation_func: Literal[\"relu\", \"sigmoid\", \"tanh\"]='relu') -> None:\n",
    "        \"\"\"\n",
    "        :param layers: Las capas de la red neuronal. La primera capa representa es la capa de entrada\n",
    "            y la última es la capa de salida.\n",
    "        :param activation_func: Función de activación a utilizar en todas las capas excepto en la de\n",
    "            salida.\n",
    "        \"\"\"\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        self._activation_func = activation_func\n",
    "        # TODO: Inicializa las matrices de pesos y vectores de sesgos utilizando \n",
    "        #     np.random.randn. Agrega en orden las matrices a los vectores a self._weights\n",
    "        #     y a self._biases.\n",
    "\n",
    "    def _activation(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Aplica la función de activación configurada.\n",
    "        :param x: Arreglo de NumPy de dimensiones (N,). Representa los puntos a evaluar.\n",
    "        :return: Arreglo de NumPy de dimensiones (N,). Contiene la evaluación de la función.\n",
    "        :raises ValueError: Cuando la función de activación no está soportada.\n",
    "        \"\"\"\n",
    "        # TODO: Completa la función siguiendo su docstring.\n",
    "        ...\n",
    "\n",
    "    def forward_propagation(self, x: np.ndarray) -> Tuple[float, List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"Propagación hacia adelante de la neurona.\n",
    "        :param x: Datos de entrada a propagar.\n",
    "        :return: Una tupla con 3 elementos. El primero la salida de la neurona, el segundo la lista\n",
    "            de activaciones y el tercero la lista de estados ocultos. Estos tres elementos se usarán\n",
    "            en la retropropagación para el cómputo de los gradientes.\n",
    "        \"\"\"\n",
    "        # TODO: Realiza la propagación hacia adelante de la neurona. \n",
    "        # NOTA: Recuerda que no debes de aplicar la función de activación en la última capa.\n",
    "        ...\n",
    "\n",
    "    def gradient_computation(\n",
    "        self,\n",
    "        output: float,\n",
    "        target: float,\n",
    "        x: np.ndarray,\n",
    "        activations: List[np.ndarray],\n",
    "        hidden_states: List[np.ndarray]\n",
    "    ) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"Calcula los gradientes para actualizar los pesos.\n",
    "        :param output: Salida de la propagación hacia adelante.\n",
    "        :param target: Valor objetivo o real a estimar.\n",
    "        :param x: Datos de entrada con los que se propagó la red.\n",
    "        :param activations: Activaciones resultado de la propagación hacia adelante.\n",
    "        :param hidden_states: Estados ocultos resultado de la propagación hacia adelante.\n",
    "        :return: Un diccionario con dos llaves 'weights' y 'biases'. El valor de cada una listas\n",
    "            de estas es un arreglo de NumPy que contienen los gradientes de los pesos y los sesgos\n",
    "            respectivamente.\n",
    "        \"\"\"\n",
    "        # TODO: Completa la función\n",
    "\n",
    "        # PASO 1: Inicializa el diccionario con los gradientes a utilizar.\n",
    "        # NOTA: Utiliza np.zeros_like para inicializar los gradientes.\n",
    "        gradients = ...\n",
    "\n",
    "        # PASO 2: Calcula la derivada del ECM con respecto a la salida de la red.\n",
    "        # NOTA: Recuerda que es para un solo punto i.e. n=1\n",
    "        loss_derivative = ...\n",
    "\n",
    "        # PASO 3: Actualiza los gradientes de la última capa\n",
    "        gradients[\"weights\"][-1] = ...\n",
    "        gradients[\"biases\"][-1] = ...\n",
    "\n",
    "        # Paso 4: Calcula los gradientes para el resto de las capas.\n",
    "        # Itera en un ciclo for para el resto de las capas. En cada iteración:\n",
    "        #     - Calcula el gradiente de h.\n",
    "        #     - Calcula el gradiente de a. (Utiliza las derivadas de las funciones de activación que programaste).\n",
    "        #     - Calcula el gradiente de w. (Recuerda utilizar x en lugar del estado oculto para la primera capa)\n",
    "        #     - Actualiza el diccionario de gradientes.\n",
    "        return gradients\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "nn = FullyConnectedNeuralNetwork(layers=[3, 5, 1], activation_func='relu')\n",
    "input_data = np.random.randn(3, 1)  # Ejemplo de entrada\n",
    "output = nn.forward_propagation(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15e581-d847-424e-b2e3-6a7afe419eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
